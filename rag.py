import logging
import os
import tempfile
from datetime import datetime, timedelta
from typing import List, Dict

import telebot
from langchain.chains.summarize import load_summarize_chain
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader, TextLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import ChatOllama
from telebot.types import InlineKeyboardMarkup

from db import add_document, get_current_week_documents

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RAG:
    def __init__(self):
        logger.info('Initializing ML models')
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        self.llm = ChatOllama(
            model="gemma3:4b",
            temperature=0,
            validate_model_on_init=True
            # openai_api_key=os.getenv('OPENAI_API_KEY')
        )
        self.vectorstore = None
        self.load_vectorstore()
        logger.info('Initializing complete')

    def load_vectorstore(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        self.vectorstore = Chroma(
            persist_directory="./chroma_db",
            embedding_function=self.embeddings
        )

    def get_retriever(self, user_id: int, k: int = 5):
        """–°–æ–∑–¥–∞–Ω–∏–µ retriever —Å —Ñ–∏–ª—å—Ç—Ä–æ–º –ø–æ user_id
        –¢—É—Ç –µ—Å—Ç—å –¥—Ä—É–≥–∏–µ –ø—Ä–∏–º–µ—Ä—ã:
        https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html#langchain_core.vectorstores.base.VectorStore.as_retriever
        """

        return self.vectorstore.as_retriever(
            # search_type="similarity_score_threshold",
            search_type="mmr",
            search_kwargs={
                "k": k,
                "filter": {"user_id": user_id},
                #"score_threshold": 0.7
            },
        )

    def query_to_rag(self, question: str, user_id: int) -> Dict:
        logger.info('query_to_rag')
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ RAG —Å–∏—Å—Ç–µ–º–µ"""
        # –ü–æ–ª—É—á–∞–µ–º retriever –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        retriever = self.get_retriever(user_id)
        docs = retriever.invoke(question)

        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é
        unique_contents = set()
        unique_docs = []

        for doc in docs:
            if doc.page_content not in unique_contents:
                unique_contents.add(doc.page_content)
                unique_docs.append(doc)

        context = "\n\n".join(doc.page_content for doc in unique_docs)

        prompt_template = f"""–ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω—ã–µ –Ω–∏–∂–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. 
        –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞, –≤–µ–∂–ª–∏–≤–æ —Å–æ–æ–±—â–∏, —á—Ç–æ –Ω–µ –º–æ–∂–µ—à—å –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

        –ö–æ–Ω—Ç–µ–∫—Å—Ç:
        {context}

        –í–æ–ø—Ä–æ—Å: {question}
        –¢—â–∞—Ç–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç:
"""
        ai_msg = self.llm.invoke(prompt_template)
        return {"answer": ai_msg.content, "documents": unique_docs}

    def add_documents_to_vectorstore(self, documents: List[Document]):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
        logger.info('add_documents_to_vectorstore')
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        )
        chunks = text_splitter.split_documents(documents)

        # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        self.vectorstore.add_documents(chunks)


class DocumentRAGBot(RAG):
    def __init__(self, telegram_token: str):
        super().__init__()
        self.bot = telebot.TeleBot(telegram_token)
        self.vector_store = None
        self.setup_handlers()

    def setup_handlers(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –∫–æ–º–∞–Ω–¥"""

        @self.bot.message_handler(commands=['start', 'help'])
        def send_welcome(message):
            welcome_text = """
ü§ñ *–í–∞—Å –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—É–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ —Ü–∏—Ñ—Ä–æ–≤—ã–º –Ω–∞—Ö–æ–¥–∫–∞–º. –î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å!*

*–Ø –º–æ–≥—É:*
üåê –ü–∞—Ä—Å–∏—Ç—å –≤–µ–±-—Å–∞–π—Ç—ã –∏ –≤–∞—à–∏ –∑–∞–º–µ—Ç–∫–∏
üîç –í—ã–ø–æ–ª–Ω—è—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
üìä –°–æ–∑–¥–∞–≤–∞—Ç—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
üìÖ –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å –æ—Ç—á–µ—Ç—ã –∑–∞ –Ω–µ–¥–µ–ª—é

*–ü–æ–¥ –∫–∞–ø–æ—Ç–æ–º*
–≠–º–±–µ–¥–¥–∏–Ω–≥–∏: [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2)
–Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å: [gemma3:4b](https://ollama.com/library/gemma3:4b)

*–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:*
/start - –ù–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É
/help - –ü–æ–º–æ—â—å
/search - –ü–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º 
/weekly\_report - –û—Ç—á–µ—Ç –ø–æ –Ω–∞—Ö–æ–¥–∫–∞–º –∑–∞ –Ω–µ–¥–µ–ª—é
"""
            self.bot.reply_to(message, welcome_text, parse_mode='Markdown', disable_web_page_preview=True)

        @self.bot.message_handler(commands=['search'])
        def handle_search(message):
            msg = self.bot.reply_to(message, "–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å:")
            self.bot.register_next_step_handler(msg, self.process_search)

        @self.bot.message_handler(commands=['weekly_report'])
        def handle_weekly_report(message):
            self.generate_weekly_report(message)

        @self.bot.message_handler(content_types=['text', 'document'])
        def handle_message(message):
            if message.text and (message.text.startswith('http://') or message.text.startswith('https://')):
                self.process_url(message)
            elif message.document:
                self.bot.reply_to(message, "–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —Å—Ç–∞–¥–∏–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏")
                # self.process_document(message)
            else:
                self.process_text(message)


    def process_text(self, message: telebot.types.Message):
        from telebot.util import quick_markup
        print(message)
        # –°–æ–∑–¥–∞–µ–º –∫–ª–∞–≤–∏–∞—Ç—É—Ä—É –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
        confirmation_id = f"{message.chat.id}_{message.from_user.id}_{message.message_id}"
        print(confirmation_id)
        TEXT = message.text
        markup = quick_markup({
            "‚úÖ –î–∞, –¥–æ–±–∞–≤–∏—Ç—å": {'callback_data': f"confirm_{confirmation_id}"},
            '‚ùå –ù–µ—Ç, –æ—Ç–º–µ–Ω–∏—Ç—å': {'callback_data': f"cancel_{confirmation_id}"}
        }, row_width=2)
        self.bot.reply_to(message, "–í—ã –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Ö–æ—Ç–∏—Ç–µ –¥–æ–±–∞–≤–∏—Ç—å –¥–∞–Ω–Ω—É—é –∑–∞–º–µ—Ç–∫—É –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π?", reply_markup=markup)

        @self.bot.callback_query_handler(func=lambda call: 'cancel' in call.data)
        def cancel_btn(call):
            markup = InlineKeyboardMarkup()
            # markup.add(InlineKeyboardButton("–ö–Ω–æ–ø–∫–∞ 2", callback_data="but_2"))
            self.bot.edit_message_reply_markup(chat_id=call.message.chat.id, message_id=call.message.id, reply_markup=markup)
            self.bot.edit_message_text("–û–ø–µ—Ä–∞—Ü–∏—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∑–∞–º–µ—Ç–∫–∏ –æ—Ç–º–µ–Ω–µ–Ω–∞", chat_id=call.message.chat.id, message_id=call.message.id)

        @self.bot.callback_query_handler(func=lambda call: 'confirm' in call.data)
        def confirm_btn(call):
            self.bot.send_chat_action(message.chat.id, 'typing')
            process_message = self.bot.edit_message_text(chat_id=call.message.chat.id, message_id=call.message.id,
                                                    text="–ò–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–º–µ—Ç–∫–∏...")

            _, message_chat_id, message_from_user_id, message_id = call.data.split('_')
            try:
                # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã
                document = Document(TEXT)
                print(document)
                document.metadata['user_id'] = message.from_user.id
                document.metadata['added_date'] = datetime.now().isoformat()
                document.metadata['summary'] = self.summarize_document(document, message)

                # add doc to db
                add_document(
                    user_id = document.metadata.get('user_id', ''),
                    added_date=datetime.fromisoformat(document.metadata.get('added_date', '')),
                    source=f'https://t.me/c/{message_chat_id}/{message_id}',
                    title=document.metadata.get('title', ''),
                    all_content=document.page_content,
                    summarize=document.metadata.get('summary', '')
                )
                # TODO: make semantic search
                self.add_documents_to_vectorstore([document])
                response = f"‚úÖ –ó–∞–º–µ—Ç–∫–∞ —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–∞!\n\nüìù –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:\n{document.metadata['summary']}"
                self.bot.reply_to(message, response)

            except Exception as e:
                logger.error(f"Error processing URL: {e}")
                self.bot.reply_to(message, f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–º–µ—Ç–∫–∏: {str(e)}")
            finally:
                # —É–¥–∞–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–µ
                self.bot.delete_message(chat_id=message.chat.id, message_id=process_message.id)

    def process_url(self, message: telebot.types.Message):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ URL"""
        url = message.text
        logger.info(f'Process url {url}')
        self.bot.send_chat_action(message.chat.id, 'typing')
        process_message = self.bot.send_message(chat_id=message.chat.id, reply_to_message_id=message.id,
                                                text="–ò–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã
            loader = WebBaseLoader(url)
            documents = loader.load()
            if documents:
                assert 1 == len(documents)
                document = documents[0]

                document.metadata['user_id'] = message.from_user.id
                document.metadata['added_date'] = datetime.now().isoformat()

                document.metadata['summary'] = self.summarize_document(document, message)

                if not document.page_content.replace('\n', '').replace('\t', '').replace(' ', ''):
                    self.bot.reply_to(message, f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å —Å–∞–π—Ç (–ø—É—Å—Ç–æ–π page_content) {url}")
                    return

                # add doc to db
                add_document(
                    user_id=document.metadata.get('user_id', ''),
                    added_date=datetime.fromisoformat(document.metadata.get('added_date', '')),
                    source=document.metadata.get('source', ''),
                    title=document.metadata.get('title', ''),
                    all_content=document.page_content,
                    summarize=document.metadata.get('summary', '')
                )

                # TODO: make semantic search
                self.add_documents_to_vectorstore([document])

                response = f"‚úÖ –°–∞–π—Ç —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω!\n\nüìù –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:\n{document.metadata['summary']}"
                self.bot.reply_to(message, response)
            else:
                self.bot.reply_to(message, "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å —Å–∞–π—Ç–∞")
        except Exception as e:
            logger.error(f"Error processing URL: {e}")
            self.bot.reply_to(message, f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–∞–π—Ç–∞: {str(e)}")
        finally:
            # —É–¥–∞–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–µ
            self.bot.delete_message(chat_id=message.chat.id, message_id=process_message.id)

    def summarize_document(self, document: Document, message: telebot.types.Message) -> str:
        """–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            # –†—É—Å—Å–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            map_prompt = """
            –°–¥–µ–ª–∞–π –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞:
            {text}
            –ö–†–ê–¢–ö–û–ï –°–û–î–ï–†–ñ–ê–ù–ò–ï:
            """
            map_prompt_template = PromptTemplate(template=map_prompt, input_variables=["text"])

            combine_prompt = """
            –°–¥–µ–ª–∞–π –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞, —Å–æ—Ö—Ä–∞–Ω—è—è –∫–ª—é—á–µ–≤—ã–µ –∏–¥–µ–∏. –°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–π –Ω–µ –±–æ–ª–µ–µ —á–µ–º –≤ 4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).
            –¢–∞–∫–∂–µ –Ω–µ –Ω—É–∂–Ω–æ –≤—Å—Ç–∞–≤–ª—è—Ç—å –≤–≤–æ–¥–Ω—ã–µ —Ñ—Ä–∞–∑—ã –ø–µ—Ä–µ–¥ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–µ–π. –°—Ä–∞–∑—É –≥–µ–Ω–µ—Ä–∏—Ä—É–π —Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:
            {text}
            –ü–û–õ–ù–û–ï –ö–†–ê–¢–ö–û–ï –°–û–î–ï–†–ñ–ê–ù–ò–ï:
            """
            combine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=["text"])

            # –ó–∞–≥—Ä—É–∑–∫–∞ —Ü–µ–ø–∏ —Å —Ä—É—Å—Å–∫–∏–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏
            chain = load_summarize_chain(
                self.llm,
                chain_type="map_reduce",
                map_prompt=map_prompt_template,
                combine_prompt=combine_prompt_template,
                # verbose=True
            )
            summary = chain.invoke({"input_documents": [document]}, return_only_outputs=True)
            return summary['output_text']

        except Exception as e:
            logger.warning(f"Summarization failed: {e}")
            self.bot.reply_to(message, "–ù–µ —É–¥–∞–ª–æ—Å—å —Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç")
            return '–ù–µ —É–¥–∞–ª–æ—Å—å —Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç'

    def generate_weekly_report(self, message: telebot.types.Message):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –∑–∞ –Ω–µ–¥–µ–ª—é"""
        try:
            # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–µ–¥–µ–ª—é
            all_docs_current_week = get_current_week_documents(user_id=message.from_user.id)
            docs_current_week_set = set()
            docs_current_week = []
            for doc in all_docs_current_week:
                if doc.source in docs_current_week_set:
                    continue
                docs_current_week_set.add(doc.source)
                docs_current_week.append(doc)

            if not docs_current_week:
                self.bot.reply_to(message, "–ó–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–µ–¥–µ–ª—é –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –¥–æ–±–∞–≤–ª—è–ª–∏—Å—å")
                return

            # –í—ã–≤–æ–¥–∏–º –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π —Å–ø–∏—Å–æ–∫
            texts = format_documents_compact(docs_current_week)
            messages = texts.split('üìÑ')
            idx_report_message = self.bot.reply_to(message, text=messages[0], parse_mode="Markdown", disable_web_page_preview=True)
            for i, text in enumerate(messages[1:]):
                if len(text) > 4096:
                    text = text[:4000] + '...'
                try:
                    self.bot.reply_to(idx_report_message, text=text, parse_mode="Markdown", disable_web_page_preview=True)
                except Exception as e:
                    logger.warning(f'Dont perse in markdown mode message:\n{text}')
                    self.bot.reply_to(idx_report_message, text=text, disable_web_page_preview=True)

        except Exception as e:
            logger.error(f"Error generating weekly report: {e}")
            self.bot.reply_to(message, f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: {str(e)}")

    def process_search(self, message: telebot.types.Message):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        try:
            query = message.text
            self.bot.send_chat_action(message.chat.id, 'typing')

            # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            res = self.query_to_rag(question=query, user_id=message.from_user.id)
            rag_answer, docs = res['answer'], res['documents']
            if not docs:
                self.bot.reply_to(message, "–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                return

            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'\n\n‚û°Ô∏è –û—Ç–≤–µ—Ç –õ–õ–ú: {rag_answer}\n"

            source_line = '‚ÑπÔ∏è–ò—Å—Ç–æ—á–Ω–∏–∫–∏:\n'
            set_of_sources = set()
            idx_source = 1
            for i, doc in enumerate(docs):
                source = doc.metadata.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫')
                if source in set_of_sources:
                    continue
                source_line += f"[{idx_source}]üìÑ - {source}\n"
                set_of_sources.add(source)
                idx_source += 1

            response += source_line
            self.bot.reply_to(message, response)

        except Exception as e:
            logger.error(f"Error in search: {e}")
            self.bot.reply_to(message, f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {str(e)}")

    def process_document(self, message):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        logger.info('Run process_document')
        self.bot.send_chat_action(message.chat.id, 'typing')
        process_message = self.bot.send_message(chat_id=message.chat.id, reply_to_message_id=message.id, text="–ò–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞...")

        try:
            file_info = self.bot.get_file(message.document.file_id)
            downloaded_file = self.bot.download_file(file_info.file_path)

            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file_info.file_path)[1]) as tmp_file:
                tmp_file.write(downloaded_file)
                tmp_file_path = tmp_file.name

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º
            if message.document.mime_type == 'application/pdf' or tmp_file_path.endswith('.pdf'):
                loader = PyPDFLoader(tmp_file_path)
            else:
                loader = TextLoader(tmp_file_path)

            documents = loader.load()

            if documents:
                assert 1 == len(documents)
                document = documents[0]
                # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ

                document.metadata['user_id'] = message.from_user.id
                document.metadata['added_date'] = datetime.now().isoformat()

                document.metadata['summary'] = self.summarize_document(document, message)

                # add doc to db
                add_document(
                    user_id=document.metadata.get('user_id', ''),
                    added_date=datetime.fromisoformat(document.metadata.get('added_date', '')),
                    source=document.metadata.get('source', ''),
                    title=document.metadata.get('title', ''),
                    all_content=document.page_content,
                    summarize=document.metadata.get('summary', '')
                )

                response = f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω!\n\nüìù –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:\n{document.metadata['summary']}"
                self.bot.reply_to(message, response)
            else:
                self.bot.reply_to(message, "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç")

            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            os.unlink(tmp_file_path)

        except Exception as e:
            logger.error(f"Error processing document: {e}")
            self.bot.reply_to(message, f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {str(e)}")
        finally:
            # —É–¥–∞–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–µ
            self.bot.delete_message(chat_id=message.chat.id, message_id=process_message.id)

    def run(self):
        """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞"""
        logger.info("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω...")
        self.bot.infinity_polling()


def format_documents_compact(documents):
    if not documents:
        return "üìÇ –£ –≤–∞—Å –ø–æ–∫–∞ –Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
    week_ago = datetime.now() - timedelta(days=7)
    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    lines = [f"üìÖ –û—Ç—á–µ—Ç –∑–∞ –Ω–µ–¥–µ–ª—é (—Å {week_ago.strftime('%d.%m.%Y')})", f"üìä –î–æ–±–∞–≤–ª–µ–Ω–æ –Ω–∞—Ö–æ–¥–æ–∫: {len(documents)}\n\n"]

    for i, doc in enumerate(documents, 1):
        lines.append(
            f"üìÑ/{i}/ - *{doc.title}*\n"
            f"üìÖ –î–∞—Ç–∞: {doc.added_date}\n"
            f"üè∑Ô∏è –ò—Å—Ç–æ—á–Ω–∏–∫: {doc.source}\n"
            f"üìù –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {doc.summarize}\n"
        )
    text = "\n".join(lines)
    return text
